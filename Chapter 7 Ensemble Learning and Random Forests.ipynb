{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adcfb986",
   "metadata": {},
   "source": [
    "# CHAPTER 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50509f18",
   "metadata": {},
   "source": [
    "# Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04966f0",
   "metadata": {},
   "source": [
    "1. if you aggregate\n",
    "the predictions of a group of predictors (such as classifiers or regressors), \n",
    "2. you will\n",
    "often get better predictions than with the best individual predictor.\n",
    "3. A group of pre‐\n",
    "dictors is called an ensemble; thus, \n",
    "4. this technique is called Ensemble Learning, and \n",
    "5. an\n",
    "Ensemble Learning algorithm is called an Ensemble method\n",
    "\n",
    "1. an ensemble of Decision Trees is called a Random Forest,\n",
    "2. you can train a group of Decision Tree classifiers, each on a different\n",
    "random subset of the training set. \n",
    "3. To make predictions, you just obtain the predic‐\n",
    "tions of all individual trees, then predict the class that gets the most votes\n",
    "\n",
    "1. Ensemble methods, including:\n",
    "1. bagging, \n",
    "3. boosting, \n",
    "4. stacking, and a few others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d182d6",
   "metadata": {},
   "source": [
    "# hard voting classifier \n",
    "1. aggregate the predictions of\n",
    "each classifier and predict the class that gets the most votes. This majority-vote classi‐\n",
    "fier is called a hard voting classifier\n",
    "2. suppose you build an ensemble containing 1,000 classifiers that are individ‐\n",
    "ually correct only 51% of the time (barely better than random guessing). \n",
    "3. If you pre‐\n",
    "dict the majority voted class, you can hope for up to 75% accuracy! \n",
    "4. However, this is\n",
    "only true if all classifiers are perfectly independent, making uncorrelated errors,\n",
    "5. which is clearly not the case since they are trained on the same data. They are likely to\n",
    "make the same types of errors,\n",
    "6. Ensemble methods work best when the predictors are as independ‐\n",
    "ent from one another as possible. One way to get diverse classifiers\n",
    "is to train them using very different algorithms. This increases the\n",
    "chance that they will make very different types of errors, improving\n",
    "the ensemble’s accuracy.\n",
    "\n",
    "\n",
    "following code creates and trains a voting classifier in Scikit-Learn, composed of\n",
    "three diverse classifiers (the training set is the moons dataset,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e592780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde4f31",
   "metadata": {},
   "source": [
    "Note: to be future-proof, we set solver=\"lbfgs\", n_estimators=100, and gamma=\"scale\" since these will be the default values in upcoming Scikit-Learn versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a3518e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lg', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('sc', SVC(random_state=42))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#hard voting\n",
    "lg = LogisticRegression(random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "sc = SVC(random_state=42)\n",
    "\n",
    "vc =  VotingClassifier(\n",
    "          [('lg', lg),('rf',rf),('sc',sc)], voting= 'hard'\n",
    ")\n",
    "\n",
    "\n",
    "vc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "567854d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "#check accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "for i in (lg,rf,sc,vc):\n",
    "    i.fit(X_train, y_train)\n",
    "    y_pred = i.predict(X_test)\n",
    "    print(i.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84354a50",
   "metadata": {},
   "source": [
    "soft voting\n",
    "\n",
    "1. If all classifiers are able to estimate class probabilities (i.e., they have a pre\n",
    "dict_proba() method), \n",
    "2. then you can tell Scikit-Learn to predict the class with the\n",
    "highest class probability, averaged over all the individual classifiers. \n",
    "3. This is called so\n",
    "voting. \n",
    "4. It often achieves higher performance than hard voting because it gives more\n",
    "weight to highly confident votes. \n",
    "5. ensure that all classifiers can estimate class probabilities. \n",
    "6. This is\n",
    "not the case of the SVC class by default, so you need to set its probability hyperpara‐\n",
    "meter to True (this will make the SVC class use cross-validation to estimate class prob‐\n",
    "abilities, slowing down training, and it will add a predict_proba() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cfe766a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lg', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('sc', SVC(probability=True, random_state=42))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#soft voting\n",
    "lg = LogisticRegression(random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "sc = SVC(random_state=42, probability= True)\n",
    "\n",
    "vc =  VotingClassifier(\n",
    "          [('lg', lg),('rf',rf),('sc',sc)], voting= 'soft'\n",
    ")\n",
    "\n",
    "\n",
    "vc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4160e15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "#check accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "for i in (lg,rf,sc,vc):\n",
    "    i.fit(X_train, y_train)\n",
    "    y_pred = i.predict(X_test)\n",
    "    print(i.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d1d57",
   "metadata": {},
   "source": [
    "# Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa011aae",
   "metadata": {},
   "source": [
    "1. One way to get a diverse set of classifiers is to use very different training algorithms,\n",
    "as just discussed. \n",
    "2. Another approach is to use the same training algorithm for every\n",
    "predictor, but to train them on different random subsets of the training set \n",
    "\n",
    "Bagging and Pasting\n",
    "\n",
    "3. When\n",
    "sampling is performed with replacement, this method is called bagging1\n",
    "(short for\n",
    "bootstrap aggregating2\n",
    "). \n",
    "4. When sampling is performed without replacement, it is called\n",
    "pasting.\n",
    "5. both bagging and pasting allow training instances to be sampled sev‐\n",
    "eral times across multiple predictors, but only bagging allows training instances to be\n",
    "sampled several times for the same predictor. \n",
    "6. In statistics, resampling with replacement is called bootstrapping\n",
    "7. Once all predictors are trained, the ensemble can make a prediction for a new\n",
    "instance by simply aggregating the predictions of all predictors.\n",
    "8. \n",
    "\n",
    "benifits of bootstraping\n",
    "\n",
    "1. each individual predictor has a higher bias than if it were trained on the original training set, but\n",
    "aggregation reduces both bias and variance.\n",
    "2. Generally, the net result is that the\n",
    "ensemble has a similar bias but a lower variance than a single predictor trained on the\n",
    "original training set.\n",
    "3. predictors can all be trained in parallel, via different\n",
    "CPU cores or even different servers. Similarly, predictions can be made in parallel.\n",
    "4. This is one of the reasons why bagging and pasting are such popular methods: they\n",
    "scale very well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0490b35",
   "metadata": {},
   "source": [
    "Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\n",
    "sifier class (or BaggingRegressor for regression)\n",
    "\n",
    "1. following code trains an\n",
    "ensemble of 500 Decision Tree classifiers,\n",
    "each trained on 100 training instances ran‐\n",
    "domly sampled from the training set with replacement (this is an example of bagging,\n",
    "\n",
    "2. but if you want to use pasting instead, just set bootstrap=False). \n",
    "3. The n_jobs param‐\n",
    "eter tells Scikit-Learn the number of CPU cores to use for training and predictions\n",
    "4. (–1 tells Scikit-Learn to use all available cores):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61931c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bg = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, max_samples=100, \n",
    "    bootstrap=True, n_jobs=-1)\n",
    "bg.fit(X_train,y_train)\n",
    "y_pred = bg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41a2a1",
   "metadata": {},
   "source": [
    "The BaggingClassifier automatically performs soft voting\n",
    "instead of hard voting if the base classifier can estimate class proba‐\n",
    "bilities (i.e., if it has a predict_proba() method), which is the case\n",
    "with Decision Trees classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f58e18a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d9e9ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0edaf09",
   "metadata": {},
   "source": [
    "As you can see, the ensemble’s predictions will likely\n",
    "generalize much better than the single Decision Tree’s predictions: the ensemble has a\n",
    "comparable bias but a smaller variance (it makes roughly the same number of errors\n",
    "on the training set, but the decision boundary is less irregular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13edf5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Bootstrapping introduces a bit more diversity in the subsets that each predictor is\n",
    "trained on, so bagging ends up with a slightly higher bias than pasting, but this also\n",
    "means that predictors end up being less correlated so the ensemble’s variance is\n",
    "reduced. Overall, bagging often results in better models, which explains why it is gen‐\n",
    "erally preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "524f59f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bg = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, max_samples=375, \n",
    "    bootstrap=True, n_jobs=-1)\n",
    "bg.fit(X_train,y_train)\n",
    "y_pred = bg.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acf97def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3e20d",
   "metadata": {},
   "source": [
    "# Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dcb35c",
   "metadata": {},
   "source": [
    "\n",
    "1. With bagging, some instances may be sampled several times for any given predictor,\n",
    "while others may not be sampled at all. \n",
    "2. By default a BaggingClassifier samples m\n",
    "training instances with replacement (bootstrap=True), where m is the size of the\n",
    "training set. \n",
    "3. This means that only about 63% of the training instances are sampled on\n",
    "average for each predictor.6\n",
    "\n",
    "out-of-bag (oob) instances\n",
    "\n",
    "4. The remaining 37% of the training instances that are not\n",
    "sampled are called out-of-bag (oob) instances. Note that they are not the same 37%\n",
    "for all predictors.\n",
    "\n",
    "5. Since a predictor never sees the oob instances during training, it can be evaluated on\n",
    "these instances, without the need for a separate validation set. \n",
    "6. You can evaluate the\n",
    "ensemble itself by averaging out the oob evaluations of each predictor.\n",
    "\n",
    "n Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to\n",
    "request an automatic oob evaluation after training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6288c877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9013333333333333"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,oob_score=True,\n",
    "    bootstrap=True, n_jobs=-1)\n",
    "bg.fit(X_train,y_train)\n",
    "\n",
    "bg.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd3b967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904\n"
     ]
    }
   ],
   "source": [
    "#test accuracy\n",
    "y_pred = bg.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24dbe0f",
   "metadata": {},
   "source": [
    "1. The oob decision function for each training instance is also available through the\n",
    "oob_decision_function_ variable. \n",
    "2. In this case (since the base estimator has a pre\n",
    "dict_proba() method) the decision function returns the class probabilities for each\n",
    "training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8132310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35449735, 0.64550265],\n",
       "       [0.34730539, 0.65269461],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [0.09659091, 0.90340909],\n",
       "       [0.34078212, 0.65921788],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98387097, 0.01612903],\n",
       "       [0.96756757, 0.03243243],\n",
       "       [0.77966102, 0.22033898],\n",
       "       [0.        , 1.        ],\n",
       "       [0.7761194 , 0.2238806 ],\n",
       "       [0.87165775, 0.12834225],\n",
       "       [0.95027624, 0.04972376],\n",
       "       [0.03743316, 0.96256684],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97969543, 0.02030457],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.98802395, 0.01197605],\n",
       "       [0.03092784, 0.96907216],\n",
       "       [0.35911602, 0.64088398],\n",
       "       [0.92307692, 0.07692308],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97927461, 0.02072539],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.63068182, 0.36931818],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.19553073, 0.80446927],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0052356 , 0.9947644 ],\n",
       "       [0.34615385, 0.65384615],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.203125  , 0.796875  ],\n",
       "       [0.39325843, 0.60674157],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02051282, 0.97948718],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.98882682, 0.01117318],\n",
       "       [0.9273743 , 0.0726257 ],\n",
       "       [0.94791667, 0.05208333],\n",
       "       [0.97126437, 0.02873563],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03846154, 0.96153846],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01648352, 0.98351648],\n",
       "       [1.        , 0.        ],\n",
       "       [0.74863388, 0.25136612],\n",
       "       [0.40449438, 0.59550562],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.69109948, 0.30890052],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.88586957, 0.11413043],\n",
       "       [1.        , 0.        ],\n",
       "       [0.6       , 0.4       ],\n",
       "       [0.12264151, 0.87735849],\n",
       "       [0.625     , 0.375     ],\n",
       "       [0.89325843, 0.10674157],\n",
       "       [0.        , 1.        ],\n",
       "       [0.18421053, 0.81578947],\n",
       "       [0.86931818, 0.13068182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99473684, 0.00526316],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05084746, 0.94915254],\n",
       "       [0.02139037, 0.97860963],\n",
       "       [0.30487805, 0.69512195],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00549451, 0.99450549],\n",
       "       [0.86979167, 0.13020833],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00588235, 0.99411765],\n",
       "       [0.        , 1.        ],\n",
       "       [0.20994475, 0.79005525],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00581395, 0.99418605],\n",
       "       [0.93048128, 0.06951872],\n",
       "       [0.77300613, 0.22699387],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [1.        , 0.        ],\n",
       "       [0.22099448, 0.77900552],\n",
       "       [0.66981132, 0.33018868],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04411765, 0.95588235],\n",
       "       [0.49723757, 0.50276243],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01734104, 0.98265896],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [0.2320442 , 0.7679558 ],\n",
       "       [0.51086957, 0.48913043],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02150538, 0.97849462],\n",
       "       [0.98019802, 0.01980198],\n",
       "       [0.28205128, 0.71794872],\n",
       "       [0.90960452, 0.09039548],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.82653061, 0.17346939],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00564972, 0.99435028],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9895288 , 0.0104712 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01025641, 0.98974359],\n",
       "       [0.95336788, 0.04663212],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01570681, 0.98429319],\n",
       "       [0.18644068, 0.81355932],\n",
       "       [0.96354167, 0.03645833],\n",
       "       [0.28901734, 0.71098266],\n",
       "       [0.98913043, 0.01086957],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.73232323, 0.26767677],\n",
       "       [0.36969697, 0.63030303],\n",
       "       [0.4       , 0.6       ],\n",
       "       [0.9017341 , 0.0982659 ],\n",
       "       [0.95408163, 0.04591837],\n",
       "       [0.04255319, 0.95744681],\n",
       "       [0.82777778, 0.17222222],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01704545, 0.98295455],\n",
       "       [0.98295455, 0.01704545],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01136364, 0.98863636],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01485149, 0.98514851],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93684211, 0.06315789],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9952381 , 0.0047619 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.32544379, 0.67455621],\n",
       "       [0.28      , 0.72      ],\n",
       "       [0.00606061, 0.99393939],\n",
       "       [0.        , 1.        ],\n",
       "       [0.28191489, 0.71808511],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97883598, 0.02116402],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.68263473, 0.31736527],\n",
       "       [0.89010989, 0.10989011],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99404762, 0.00595238],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.09638554, 0.90361446],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03932584, 0.96067416],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02424242, 0.97575758],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [0.91237113, 0.08762887],\n",
       "       [0.7679558 , 0.2320442 ],\n",
       "       [0.55497382, 0.44502618],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14857143, 0.85142857],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96111111, 0.03888889],\n",
       "       [0.96923077, 0.03076923],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01604278, 0.98395722],\n",
       "       [0.        , 1.        ],\n",
       "       [0.37096774, 0.62903226],\n",
       "       [0.85380117, 0.14619883],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0049505 , 0.9950495 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96116505, 0.03883495],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24873096, 0.75126904],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97109827, 0.02890173],\n",
       "       [0.84761905, 0.15238095],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0855615 , 0.9144385 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01036269, 0.98963731],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02139037, 0.97860963],\n",
       "       [0.99418605, 0.00581395],\n",
       "       [0.80446927, 0.19553073],\n",
       "       [0.        , 1.        ],\n",
       "       [0.92473118, 0.07526882],\n",
       "       [0.98421053, 0.01578947],\n",
       "       [0.16923077, 0.83076923],\n",
       "       [0.23529412, 0.76470588],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.22395833, 0.77604167],\n",
       "       [0.96153846, 0.03846154],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99438202, 0.00561798],\n",
       "       [0.        , 1.        ],\n",
       "       [0.51052632, 0.48947368],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.11340206, 0.88659794],\n",
       "       [0.10555556, 0.89444444],\n",
       "       [0.98863636, 0.01136364],\n",
       "       [0.01604278, 0.98395722],\n",
       "       [1.        , 0.        ],\n",
       "       [0.37569061, 0.62430939],\n",
       "       [0.10795455, 0.89204545],\n",
       "       [0.57988166, 0.42011834],\n",
       "       [0.61538462, 0.38461538],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.58988764, 0.41011236],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.17204301, 0.82795699],\n",
       "       [0.80662983, 0.19337017],\n",
       "       [0.08379888, 0.91620112],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83333333, 0.16666667],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0923913 , 0.9076087 ],\n",
       "       [0.02717391, 0.97282609],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99462366, 0.00537634],\n",
       "       [0.9375    , 0.0625    ],\n",
       "       [0.17204301, 0.82795699],\n",
       "       [0.97687861, 0.02312139],\n",
       "       [0.01086957, 0.98913043],\n",
       "       [0.57386364, 0.42613636],\n",
       "       [0.06779661, 0.93220339],\n",
       "       [0.96045198, 0.03954802],\n",
       "       [0.86111111, 0.13888889],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93513514, 0.06486486],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21538462, 0.78461538],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.89265537, 0.10734463],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.70466321, 0.29533679],\n",
       "       [0.92592593, 0.07407407],\n",
       "       [1.        , 0.        ],\n",
       "       [0.75      , 0.25      ],\n",
       "       [0.48704663, 0.51295337],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94578313, 0.05421687],\n",
       "       [0.00537634, 0.99462366],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83425414, 0.16574586],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.78089888, 0.21910112],\n",
       "       [0.12432432, 0.87567568],\n",
       "       [0.41706161, 0.58293839],\n",
       "       [0.2371134 , 0.7628866 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.84883721, 0.15116279],\n",
       "       [0.8372093 , 0.1627907 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02150538, 0.97849462],\n",
       "       [0.96153846, 0.03846154],\n",
       "       [0.96791444, 0.03208556],\n",
       "       [1.        , 0.        ],\n",
       "       [0.50537634, 0.49462366],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9893617 , 0.0106383 ],\n",
       "       [0.02094241, 0.97905759],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96568627, 0.03431373],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07777778, 0.92222222],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0104712 , 0.9895288 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.08900524, 0.91099476],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.45856354, 0.54143646],\n",
       "       [0.06593407, 0.93406593],\n",
       "       [0.1547619 , 0.8452381 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98941799, 0.01058201],\n",
       "       [0.21354167, 0.78645833],\n",
       "       [0.98333333, 0.01666667],\n",
       "       [0.01092896, 0.98907104],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96759259, 0.03240741],\n",
       "       [0.35326087, 0.64673913],\n",
       "       [0.99418605, 0.00581395],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01694915, 0.98305085],\n",
       "       [0.98550725, 0.01449275],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01886792, 0.98113208],\n",
       "       [0.65789474, 0.34210526]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc5208",
   "metadata": {},
   "source": [
    "# Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e0088",
   "metadata": {},
   "source": [
    "1. The BaggingClassifier class supports sampling the features as well. \n",
    "2. This is con‐\n",
    "trolled by two hyperparameters: max_features and bootstrap_features. \n",
    "3. They work\n",
    "the same way as max_samples and bootstrap, \n",
    "4. but for feature sampling instead of\n",
    "instance sampling. Thus, \n",
    "5. each predictor will be trained on a random subset of the\n",
    "input features.\n",
    "6. This is particularly useful when you are dealing with high-dimensional inputs (such\n",
    "as images). \n",
    "7. Sampling both training instances and features is called the Random\n",
    "Patches method.\n",
    "\n",
    "\n",
    "8. Keeping all training instances (i.e., bootstrap=False and max_sam\n",
    "ples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_fea\n",
    "tures smaller than 1.0) is called the Random Subspaces method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68d3b888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bg = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, max_samples=1.0, \n",
    "    bootstrap=False, n_jobs=-1,bootstrap_features=True,max_features=1.0 )\n",
    "bg.fit(X_train,y_train)\n",
    "y_pred = bg.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d44f7",
   "metadata": {},
   "source": [
    "Sampling features results in even more predictor diversity, trading a bit more bias for\n",
    "a lower variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247d7cf",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ee4cd",
   "metadata": {},
   "source": [
    "1. Random forests provide an improvement over bagged trees by way of a random\n",
    "small tweak that decorrelates the trees. \n",
    "2. As in bagging, we build a number forest\n",
    "of decision trees on bootstrapped training samples. But when building these\n",
    "decision trees, each time a split in a tree is considered, a random sample of\n",
    "m predictors is chosen as split candidates from the full set of p predictors.\n",
    "3. The split is allowed to use only one of those m predictors. \n",
    "4. A fresh sample of\n",
    "m predictors is taken at each split, and typically we choose m ≈ √p—that\n",
    "is, the number of predictors considered at each split is approximately equal\n",
    "to the square root of the total number of predictors (4 out of the 13 for the\n",
    "Heart data).\n",
    "\n",
    "5. In other words, in building a random forest, at each split in the tree,\n",
    "the algorithm is not even allowed to consider a majority of the available\n",
    "predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb11d7",
   "metadata": {},
   "source": [
    "1. generally\n",
    "trained via the bagging method (or sometimes pasting), typically with max_samples\n",
    "set to the size of the training set. \n",
    "2. Instead of building a BaggingClassifier and pass‐\n",
    "ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier\n",
    "class, \n",
    "3. which is more convenient and optimized for Decision Trees10 (similarly, there is\n",
    "a RandomForestRegressor class for regression tasks).\n",
    "\n",
    "4. With a few exceptions, a RandomForestClassifier has all the hyperparameters of a\n",
    "DecisionTreeClassifier (to control how trees are grown), plus all the hyperpara‐\n",
    "meters of a BaggingClassifier to control the ensemble itself\n",
    "\n",
    "5. The BaggingClassifier class remains useful if you want a bag of something other than Decision Trees.\n",
    "6. There are a few notable exceptions: splitter is absent (forced to \"random\"), presort is absent (forced to\n",
    "False), max_samples is absent (forced to 1.0), and base_estimator is absent (forced to DecisionTreeClassi\n",
    "fier with the provided hyperparameters).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614046a",
   "metadata": {},
   "source": [
    "1. The Random Forest algorithm introduces extra randomness when growing trees;\n",
    "2. instead of searching for the very best feature when splitting a node ,\n",
    "3. it\n",
    "searches for the best feature among a random subset of features. \n",
    "4. This results in a\n",
    "greater tree diversity, which (once again) trades a higher bias for a lower variance,\n",
    "5. generally yielding an overall better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88a8881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22237d",
   "metadata": {},
   "source": [
    "The following BaggingClassifier is\n",
    "roughly equivalent to the previous RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04c4930f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912\n"
     ]
    }
   ],
   "source": [
    "bg = BaggingClassifier(\n",
    " DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    " n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1,bootstrap_features=True,max_features=1.0)\n",
    "bg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = bg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ec701",
   "metadata": {},
   "source": [
    "# Extra-Trees\n",
    "1. When you are growing a tree in a Random Forest, at each node only a random subset\n",
    "of the features is considered for splitting (as discussed earlier). It is possible to make\n",
    "trees even more random by also using random thresholds for each feature rather than\n",
    "searching for the best possible thresholds (like regular Decision Trees do).\n",
    "A forest of such extremely random trees is simply called an Extremely Randomized\n",
    "Trees ensemble12 (or Extra-Trees for short).\n",
    "2. this trades more bias for a\n",
    "lower variance. \n",
    "3. It also makes Extra-Trees much faster to train than regular Random\n",
    "Forests since finding the best possible threshold for each feature at every node is one\n",
    "of the most time-consuming tasks of growing a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda495ab",
   "metadata": {},
   "source": [
    "You can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\n",
    "class.\n",
    "\n",
    "It is hard to tell in advance whether a RandomForestClassifier\n",
    "will perform better or worse than an ExtraTreesClassifier. Gen‐\n",
    "erally, the only way to know is to try both and compare them using\n",
    "cross-validation (and tuning the hyperparameters using grid\n",
    "search).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b86bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "et = ExtraTreeClassifier( max_leaf_nodes=16)\n",
    "et.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = et.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eea4d0",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "1. another great quality of Random Forests is that they make it easy to measure the \n",
    "relative importance of each feature. \n",
    "2. Scikit-Learn measures a feature’s importance by\n",
    "looking at how much the tree nodes that use that feature reduce impurity on average\n",
    "(across all trees in the forest). \n",
    "3. More precisely, it is a weighted average, where each\n",
    "node’s weight is equal to the number of training samples that are associated with it\n",
    "4. You can access the\n",
    "result using the feature_importances_ variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79031b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 11.249225099876375 %\n",
      "sepal width (cm) 2.311928828251033 %\n",
      "petal length (cm) 44.10304643639577 %\n",
      "petal width (cm) 42.33579963547682 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf.fit(iris[\"data\"], iris[\"target\"])\n",
    "\n",
    "for name, score in zip(iris[\"feature_names\"], (rf.feature_importances_*100)):\n",
    "    print(name, score,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c2d18",
   "metadata": {},
   "source": [
    " It seems that the most important features are the\n",
    "petal length (44%) and width (42%),\n",
    "\n",
    "Random Forests are very handy to get a quick understanding of what features\n",
    "actually matter, in particular if you need to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3914d7b",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "1. Boosting (originally called hypothesis boosting) refers to any Ensemble method that\n",
    "can combine several weak learners into a strong learner. \n",
    "The general idea of most\n",
    "2. boosting methods is to train predictors sequentially, each trying to correct its prede‐\n",
    "cessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46bfb1",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention\n",
    "to the training instances that the predecessor underfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450f55c",
   "metadata": {},
   "source": [
    "There is one important drawback to this sequential learning techni‐\n",
    "que: it cannot be parallelized (or only partially), since each predic‐\n",
    "tor can only be trained after the previous predictor has been\n",
    "trained and evaluated. As a result, it does not scale as well as bag‐\n",
    "ging or pasting.\n",
    "\n",
    "Scikit-Learn actually uses a multiclass version of AdaBoost called SAMME16 (which\n",
    "stands for Stagewise Additive Modeling using a Multiclass Exponential loss function).\n",
    "When there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\n",
    "predictors can estimate class probabilities (i.e., if they have a predict_proba()\n",
    "method), Scikit-Learn can use a variant of SAMME called SAMME.R (the R stands\n",
    "for “Real”), which relies on class probabilities rather than predictions and generally\n",
    "performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "033ff2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    " DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    " algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "If your AdaBoost ensemble is overfitting the training set, you can\n",
    "try reducing the number of estimators or more strongly regulariz‐\n",
    "ing the base estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6331e0",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "Another very popular Boosting algorithm is Gradient Boosting.\n",
    "17 Just like AdaBoost,\n",
    "Gradient Boosting works by sequentially adding predictors to an ensemble, each one\n",
    "correcting its predecessor. However, instead of tweaking the instance weights at every\n",
    "iteration like AdaBoost does, this method tries to fit the new predictor to the residual\n",
    "errors made by the previous predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892523e",
   "metadata": {},
   "source": [
    "Let’s go through a simple regression example using Decision Trees as the base predic‐\n",
    "tors (of course Gradient Boosting also works great with regression tasks). This is\n",
    "called Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT).\n",
    "Let create a simple quadratic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f49e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1630e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's train a decision tree regressor on this dataset:\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31581eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now train a second DecisionTreeRegressor on the residual errors made by the first predictor:\n",
    "y2 = y- tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b0ec7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Then we train a third regressor on the residual errors made by the second predictor:\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261846d",
   "metadata": {},
   "source": [
    "Now we have an ensemble containing three trees. It can make predictions on a new\n",
    "instance simply by adding up the predictions of all the trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbef9311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0.8]])\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c6c9b",
   "metadata": {},
   "source": [
    "A simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRe\n",
    "gressor class. Much like the RandomForestRegressor class, it has hyperparameters to\n",
    "control the growth of Decision Trees (e.g., max_depth, min_samples_leaf, and so on),\n",
    "as well as hyperparameters to control the ensemble training, such as the number of\n",
    "trees (n_estimators). The following code creates the same ensemble as the previous\n",
    "one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "756fa02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c91ad1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "gbrt_slow.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b3c17",
   "metadata": {},
   "source": [
    "The learning_rate hyperparameter scales the contribution of each tree. If you set it\n",
    "to a low value, such as 0.1, you will need more trees in the ensemble to fit the train‐\n",
    "ing set, but the predictions will usually generalize better. This is a regularization tech‐\n",
    "nique called shrinkage. Figure 7-10 shows two GBRT ensembles trained with a low\n",
    "learning rate: the one on the left does not have enough trees to fit the training set,\n",
    "while the one on the right has too many trees and overfits the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897749d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting with Early stopping:\n",
    "\n",
    "In order to find the optimal number of trees, you can use early stopping (see Chap‐\n",
    "ter 4). A simple way to implement this is to use the staged_predict() method: it\n",
    "returns an iterator over the predictions made by the ensemble at each stage of train‐\n",
    "ing (with one tree, two trees, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b84322b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "258281b3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25042018 0.25042018 0.25042018 0.25042018 0.25042018 0.25042018\n",
      " 0.25042018 0.25042018 0.30371637 0.29018668 0.25042018 0.25042018\n",
      " 0.25042018 0.25042018 0.30371637 0.25042018 0.25042018 0.25042018\n",
      " 0.30371637 0.25042018 0.25042018 0.30371637 0.25042018 0.28211232\n",
      " 0.29018668]\n",
      "[0.23313281 0.23313281 0.23313281 0.23313281 0.23313281 0.23313281\n",
      " 0.23313281 0.23313281 0.33858247 0.31049451 0.26238205 0.27072802\n",
      " 0.26238205 0.23313281 0.33858247 0.23313281 0.23313281 0.23313281\n",
      " 0.33858247 0.23313281 0.23313281 0.33858247 0.23313281 0.29407419\n",
      " 0.31049451]\n",
      "[0.21915515 0.21915515 0.21915515 0.21915515 0.21915515 0.21915515\n",
      " 0.21915515 0.21915515 0.36996196 0.33446413 0.27314773 0.25675036\n",
      " 0.27314773 0.21915515 0.36996196 0.21915515 0.21915515 0.21915515\n",
      " 0.36996196 0.21915515 0.21915515 0.36996196 0.21915515 0.30483988\n",
      " 0.33446413]\n",
      "[0.20444938 0.20444938 0.20444938 0.20444938 0.20444938 0.20444938\n",
      " 0.20444938 0.20444938 0.39820349 0.35135615 0.28268598 0.27364238\n",
      " 0.28268598 0.20444938 0.39820349 0.20444938 0.20444938 0.20444938\n",
      " 0.39820349 0.20444938 0.20444938 0.39820349 0.20444938 0.31437812\n",
      " 0.35135615]\n",
      "[0.19478113 0.19478113 0.19478113 0.19478113 0.19478113 0.19478113\n",
      " 0.19478113 0.19478113 0.41425344 0.3712396  0.29873592 0.26397412\n",
      " 0.29873592 0.19478113 0.41425344 0.19478113 0.19478113 0.19478113\n",
      " 0.41425344 0.19478113 0.19478113 0.41425344 0.19478113 0.33042806\n",
      " 0.3712396 ]\n",
      "[0.18251276 0.18251276 0.18251276 0.18251276 0.18251276 0.18251276\n",
      " 0.18251276 0.18251276 0.44050282 0.38524212 0.30915661 0.27797664\n",
      " 0.30915661 0.18251276 0.42467413 0.18251276 0.18251276 0.18251276\n",
      " 0.42467413 0.18251276 0.18251276 0.42467413 0.18251276 0.34084876\n",
      " 0.38524212]\n",
      "[0.19195261 0.19195261 0.16892636 0.19195261 0.16892636 0.16892636\n",
      " 0.16892636 0.16892636 0.46148774 0.39468197 0.31617965 0.28741649\n",
      " 0.31617965 0.16892636 0.44565905 0.16892636 0.16892636 0.16892636\n",
      " 0.44565905 0.16892636 0.19195261 0.44565905 0.1895358  0.3478718\n",
      " 0.39468197]\n",
      "[0.18191916 0.18191916 0.15889291 0.18191916 0.15889291 0.15889291\n",
      " 0.15889291 0.15889291 0.48037417 0.41023284 0.32236176 0.27738304\n",
      " 0.32236176 0.15889291 0.46454548 0.15889291 0.15889291 0.15889291\n",
      " 0.46454548 0.15889291 0.18191916 0.46454548 0.19571792 0.35405391\n",
      " 0.41023284]\n",
      "[0.19001123 0.19001123 0.14716102 0.19001123 0.14716102 0.14716102\n",
      " 0.14716102 0.14716102 0.48786511 0.41832491 0.32985271 0.28547511\n",
      " 0.32985271 0.14716102 0.47203642 0.14716102 0.14716102 0.14716102\n",
      " 0.47203642 0.14716102 0.19001123 0.47203642 0.20320886 0.36154485\n",
      " 0.41832491]\n",
      "[0.18273733 0.18273733 0.13988712 0.18273733 0.13988712 0.13988712\n",
      " 0.13988712 0.13988712 0.49708238 0.43283886 0.33906997 0.2782012\n",
      " 0.33906997 0.13988712 0.48125369 0.13988712 0.13988712 0.13988712\n",
      " 0.48125369 0.13988712 0.18273733 0.48125369 0.21242612 0.37076211\n",
      " 0.44794998]\n",
      "[0.1775144  0.1775144  0.13466419 0.1775144  0.13466419 0.13466419\n",
      " 0.13466419 0.13466419 0.50631044 0.44733596 0.33384704 0.27297828\n",
      " 0.33384704 0.13466419 0.49048175 0.13466419 0.13466419 0.13466419\n",
      " 0.49048175 0.13466419 0.1775144  0.49048175 0.20720319 0.37999018\n",
      " 0.46244708]\n",
      "[0.18424586 0.18424586 0.12633331 0.18424586 0.12633331 0.12633331\n",
      " 0.12633331 0.12633331 0.5126649  0.45406742 0.3402015  0.27970974\n",
      " 0.3402015  0.12633331 0.49683621 0.12633331 0.12633331 0.12633331\n",
      " 0.49683621 0.12633331 0.18424586 0.49683621 0.19887232 0.38634464\n",
      " 0.46917854]\n",
      "[0.17818543 0.17818543 0.12027289 0.17818543 0.12027289 0.12027289\n",
      " 0.12027289 0.12027289 0.5194516  0.46500711 0.3469882  0.27364931\n",
      " 0.3469882  0.12027289 0.50362291 0.12027289 0.12027289 0.12027289\n",
      " 0.50362291 0.12027289 0.17818543 0.50362291 0.20565902 0.39313134\n",
      " 0.49371824]\n",
      "[0.17273105 0.17273105 0.1148185  0.17273105 0.1148185  0.1148185\n",
      " 0.1148185  0.1148185  0.52555964 0.47485284 0.35309624 0.26819492\n",
      " 0.35309624 0.1148185  0.50973095 0.1148185  0.1148185  0.1148185\n",
      " 0.50973095 0.1148185  0.17273105 0.50973095 0.21176705 0.39923938\n",
      " 0.51580398]\n",
      "[0.16873282 0.16873282 0.11082027 0.16873282 0.11082027 0.11082027\n",
      " 0.11082027 0.11082027 0.53469186 0.48272308 0.34909801 0.26419669\n",
      " 0.34909801 0.11082027 0.51886317 0.11082027 0.11082027 0.11082027\n",
      " 0.51886317 0.11082027 0.16873282 0.51886317 0.20776883 0.39524115\n",
      " 0.52367421]\n",
      "[0.17374206 0.17374206 0.10321384 0.17374206 0.10321384 0.10321384\n",
      " 0.10321384 0.10321384 0.54620707 0.48773231 0.35262956 0.26920593\n",
      " 0.35262956 0.10321384 0.52239472 0.10321384 0.11582951 0.10321384\n",
      " 0.52239472 0.10321384 0.17374206 0.52239472 0.21130038 0.3987727\n",
      " 0.52868345]\n",
      "[0.16978657 0.16978657 0.09925836 0.16978657 0.09925836 0.09925836\n",
      " 0.09925836 0.09925836 0.55108707 0.49840073 0.35750956 0.26525045\n",
      " 0.35750956 0.09925836 0.52727472 0.09925836 0.11187403 0.09925836\n",
      " 0.52727472 0.09925836 0.16978657 0.52727472 0.21618038 0.40365271\n",
      " 0.53935187]\n",
      "[0.1651404  0.1651404  0.09461219 0.1651404  0.09461219 0.09461219\n",
      " 0.09461219 0.10283176 0.55466047 0.50549206 0.36108297 0.26060428\n",
      " 0.36108297 0.09461219 0.53084813 0.10283176 0.10722785 0.09461219\n",
      " 0.53084813 0.09461219 0.1651404  0.53084813 0.21975379 0.40722611\n",
      " 0.55629974]\n",
      "[0.16966122 0.16966122 0.09035966 0.16966122 0.09035966 0.09035966\n",
      " 0.09035966 0.09857923 0.55959348 0.51001287 0.35683044 0.2651251\n",
      " 0.35683044 0.09035966 0.53578113 0.09857923 0.11174867 0.09035966\n",
      " 0.53578113 0.09035966 0.16966122 0.53578113 0.21550125 0.41215912\n",
      " 0.56082056]\n",
      "[0.16510475 0.16510475 0.08580318 0.16510475 0.08580318 0.08580318\n",
      " 0.08580318 0.10180079 0.56281504 0.5154106  0.360052   0.26056863\n",
      " 0.360052   0.08580318 0.5390027  0.10180079 0.1071922  0.08580318\n",
      " 0.5390027  0.08580318 0.16510475 0.5390027  0.21872282 0.41538068\n",
      " 0.57562156]\n",
      "[0.16820751 0.16820751 0.07816496 0.16820751 0.07816496 0.07816496\n",
      " 0.07816496 0.1047002  0.56571445 0.51851336 0.36295141 0.26367139\n",
      " 0.36295141 0.07816496 0.5419021  0.1047002  0.11029496 0.07816496\n",
      " 0.5419021  0.07816496 0.16820751 0.5419021  0.22162222 0.41828009\n",
      " 0.58894246]\n",
      "[0.17173932 0.17173932 0.07488193 0.17173932 0.07488193 0.07488193\n",
      " 0.07488193 0.10141718 0.57099559 0.52204517 0.35966838 0.26720319\n",
      " 0.35966838 0.07488193 0.54718324 0.10141718 0.11382677 0.07488193\n",
      " 0.54718324 0.07488193 0.17173932 0.54718324 0.2183392  0.41499706\n",
      " 0.59247427]\n",
      "[0.17491794 0.17491794 0.06946072 0.17491794 0.06946072 0.06946072\n",
      " 0.06946072 0.09599596 0.57321218 0.52522379 0.36188497 0.27038182\n",
      " 0.36188497 0.06946072 0.54939983 0.09599596 0.1170054  0.06946072\n",
      " 0.54939983 0.06946072 0.17491794 0.54939983 0.22055579 0.41721365\n",
      " 0.5956529 ]\n",
      "[0.17207768 0.17207768 0.06662045 0.17207768 0.06662045 0.06662045\n",
      " 0.06662045 0.09848158 0.5756978  0.53140612 0.36437059 0.26754156\n",
      " 0.36437059 0.06662045 0.55188545 0.09848158 0.11416514 0.06662045\n",
      " 0.55188545 0.06662045 0.17207768 0.55188545 0.22304141 0.41969927\n",
      " 0.60183522]\n",
      "[0.16786488 0.16786488 0.06240765 0.16786488 0.06240765 0.06240765\n",
      " 0.06240765 0.10071864 0.57793486 0.53438198 0.36660765 0.27051742\n",
      " 0.36660765 0.06240765 0.55412251 0.10071864 0.10995234 0.06240765\n",
      " 0.55412251 0.06240765 0.16786488 0.55412251 0.22527847 0.42193633\n",
      " 0.61212825]\n",
      "[0.17005162 0.17005162 0.05673268 0.17005162 0.05673268 0.05673268\n",
      " 0.05673268 0.10273199 0.57994821 0.53656871 0.36862101 0.27270415\n",
      " 0.36862101 0.05673268 0.55613587 0.10273199 0.11213907 0.05673268\n",
      " 0.55613587 0.05673268 0.17005162 0.55613587 0.22729182 0.42394969\n",
      " 0.62139198]\n",
      "[0.17221443 0.17221443 0.05412411 0.17221443 0.05889549 0.05412411\n",
      " 0.05412411 0.10012342 0.58362392 0.53873152 0.36601243 0.27486696\n",
      " 0.36601243 0.05412411 0.55981158 0.10012342 0.11430188 0.05412411\n",
      " 0.55981158 0.05412411 0.17221443 0.55981158 0.22468325 0.42134111\n",
      " 0.62355478]\n",
      "[0.17416096 0.17416096 0.05154273 0.17416096 0.06084202 0.05154273\n",
      " 0.05154273 0.09754205 0.58608483 0.54067805 0.36343106 0.27681349\n",
      " 0.36343106 0.05154273 0.56227248 0.09754205 0.11624841 0.05154273\n",
      " 0.56227248 0.05154273 0.17416096 0.56227248 0.22210188 0.42380202\n",
      " 0.62550131]\n",
      "[0.17591283 0.17591283 0.04579749 0.17591283 0.06259389 0.04579749\n",
      " 0.04579749 0.09880968 0.58735246 0.54242992 0.3646987  0.27856537\n",
      " 0.3646987  0.04579749 0.56354012 0.09880968 0.11800028 0.04579749\n",
      " 0.56354012 0.04579749 0.17591283 0.56354012 0.22336952 0.42506965\n",
      " 0.62725319]\n",
      "[0.1739816  0.1739816  0.04386625 0.1739816  0.06066266 0.04386625\n",
      " 0.04386625 0.1004931  0.58903588 0.54653178 0.36638211 0.27663413\n",
      " 0.36638211 0.04386625 0.56522353 0.1004931  0.11606905 0.04386625\n",
      " 0.56522353 0.04386625 0.1739816  0.56522353 0.22505293 0.42675307\n",
      " 0.63135505]\n",
      "[0.17182177 0.17182177 0.04170643 0.17182177 0.05850283 0.04170643\n",
      " 0.04170643 0.10200817 0.59055095 0.54940627 0.36789719 0.27447431\n",
      " 0.36789719 0.04170643 0.56673861 0.10200817 0.11390923 0.04170643\n",
      " 0.56673861 0.04170643 0.17182177 0.56673861 0.22656801 0.42826814\n",
      " 0.63422954]\n",
      "[0.17335882 0.17335882 0.03730579 0.17335882 0.0541022  0.03730579\n",
      " 0.03730579 0.10317465 0.59171743 0.55094332 0.36906366 0.27601136\n",
      " 0.36906366 0.03730579 0.56790508 0.10317465 0.11544627 0.0428729\n",
      " 0.56790508 0.03730579 0.17335882 0.56790508 0.22773448 0.42943462\n",
      " 0.6406497 ]\n",
      "[0.17491625 0.17491625 0.03572899 0.17491625 0.05565963 0.03572899\n",
      " 0.03572899 0.10159785 0.5946286  0.55250075 0.36748686 0.27756879\n",
      " 0.36748686 0.03572899 0.56632828 0.10159785 0.1170037  0.0412961\n",
      " 0.56632828 0.03572899 0.17491625 0.56632828 0.22615768 0.42785781\n",
      " 0.64220713]\n",
      "[0.17631794 0.17631794 0.0338348  0.17631794 0.05706131 0.0338348\n",
      " 0.0338348  0.09970365 0.59653541 0.55390243 0.36559267 0.27897047\n",
      " 0.36559267 0.0338348  0.56823509 0.09970365 0.11840539 0.03940191\n",
      " 0.56823509 0.0338348  0.17631794 0.56823509 0.22426348 0.42976463\n",
      " 0.64360882]\n",
      "[0.17757945 0.17757945 0.03084798 0.17757945 0.05832283 0.03084798\n",
      " 0.03084798 0.09671684 0.59762907 0.55516395 0.36668632 0.28023199\n",
      " 0.36668632 0.03084798 0.56932874 0.09671684 0.11966691 0.03641509\n",
      " 0.56932874 0.03084798 0.17757945 0.56932874 0.22535714 0.43085828\n",
      " 0.64487033]\n",
      "[0.17871482 0.17871482 0.02707942 0.17871482 0.0594582  0.02707942\n",
      " 0.02707942 0.09774954 0.59866177 0.55629932 0.36771902 0.28136735\n",
      " 0.36771902 0.02707942 0.57036145 0.09774954 0.12080227 0.03264653\n",
      " 0.57036145 0.02707942 0.17871482 0.57036145 0.22638984 0.43189098\n",
      " 0.6460057 ]\n",
      "[0.17732739 0.17732739 0.025692   0.17732739 0.05807077 0.025692\n",
      " 0.025692   0.09877275 0.59968498 0.55889216 0.36874223 0.27997993\n",
      " 0.36874223 0.025692   0.57138466 0.09877275 0.11941485 0.03366975\n",
      " 0.57138466 0.025692   0.17732739 0.57138466 0.22741305 0.43291419\n",
      " 0.64859854]\n",
      "[0.17534591 0.17534591 0.02371051 0.17534591 0.05608929 0.02371051\n",
      " 0.02371051 0.09969364 0.60060587 0.56032405 0.36966312 0.27799844\n",
      " 0.36966312 0.02371051 0.57230555 0.09969364 0.11743336 0.03459064\n",
      " 0.57230555 0.02371051 0.17534591 0.57230555 0.22833394 0.43383508\n",
      " 0.65003043]\n",
      "[0.17636738 0.17636738 0.02053445 0.17636738 0.05291322 0.02053445\n",
      " 0.02053445 0.10042766 0.60133989 0.56134553 0.37039715 0.27901992\n",
      " 0.37039715 0.02444454 0.57303957 0.10042766 0.11845484 0.03532466\n",
      " 0.57303957 0.02053445 0.17636738 0.57303957 0.22906796 0.43456911\n",
      " 0.65105191]\n",
      "[0.17738189 0.17738189 0.01950496 0.17738189 0.05392773 0.01950496\n",
      " 0.01950496 0.09939817 0.6033408  0.56236004 0.36936765 0.28003443\n",
      " 0.36936765 0.02341504 0.57201007 0.09939817 0.11946935 0.03429516\n",
      " 0.57201007 0.01950496 0.17738189 0.57201007 0.22803847 0.43353961\n",
      " 0.65206642]\n",
      "[0.17829495 0.17829495 0.01742682 0.17829495 0.05484079 0.01742682\n",
      " 0.01742682 0.09732004 0.60411365 0.56327309 0.37014051 0.28094749\n",
      " 0.37014051 0.02133691 0.57278293 0.09732004 0.1203824  0.03221703\n",
      " 0.57278293 0.01742682 0.17829495 0.57278293 0.22881132 0.43431247\n",
      " 0.65297948]\n",
      "[0.1791167  0.1791167  0.01424959 0.1791167  0.05566254 0.01424959\n",
      " 0.01424959 0.09790304 0.60469666 0.56409485 0.37072351 0.28176924\n",
      " 0.37072351 0.01815968 0.57336594 0.09790304 0.12120416 0.03280004\n",
      " 0.57336594 0.01424959 0.1791167  0.57336594 0.22939433 0.43489548\n",
      " 0.65380123]\n",
      "[0.17816673 0.17816673 0.01329962 0.17816673 0.05471257 0.01329962\n",
      " 0.01329962 0.09859771 0.60539133 0.56590813 0.37141818 0.28081926\n",
      " 0.37141818 0.0172097  0.57406061 0.09859771 0.12025418 0.0334947\n",
      " 0.57406061 0.01329962 0.17816673 0.57406061 0.230089   0.43559014\n",
      " 0.65561452]\n",
      "[0.17716259 0.17716259 0.01229548 0.17716259 0.05370843 0.01229548\n",
      " 0.01229548 0.09917692 0.60597053 0.56730641 0.37199739 0.27981513\n",
      " 0.37199739 0.0177889  0.57463981 0.09917692 0.11925005 0.03407391\n",
      " 0.57463981 0.01229548 0.17716259 0.57463981 0.2306682  0.43616935\n",
      " 0.65701279]\n",
      "[0.17770213 0.17770213 0.00871777 0.17770213 0.05424797 0.00871777\n",
      " 0.00871777 0.09965791 0.60645153 0.56784594 0.37247838 0.28035467\n",
      " 0.37247838 0.0182699  0.57512081 0.09965791 0.11978958 0.03455491\n",
      " 0.57512081 0.00871777 0.17770213 0.57512081 0.2311492  0.43665034\n",
      " 0.65755233]\n",
      "[0.17842073 0.17842073 0.0080642  0.17842073 0.05496656 0.0080642\n",
      " 0.0080642  0.09900435 0.60579796 0.56856454 0.37182482 0.28107326\n",
      " 0.37182482 0.01761633 0.57446724 0.09900435 0.12050818 0.03390134\n",
      " 0.57446724 0.0080642  0.17842073 0.57446724 0.23049563 0.43599678\n",
      " 0.65827092]\n",
      "[0.17906746 0.17906746 0.00729646 0.17906746 0.0556133  0.00729646\n",
      " 0.00729646 0.0982366  0.60686633 0.56921128 0.37105707 0.28172\n",
      " 0.37105707 0.01684859 0.57553561 0.0982366  0.12115491 0.03313359\n",
      " 0.57553561 0.00729646 0.17906746 0.57553561 0.22972789 0.43522903\n",
      " 0.65891766]\n",
      "[0.17964952 0.17964952 0.00539399 0.17964952 0.05619536 0.00539399\n",
      " 0.00539399 0.09855052 0.60718025 0.56979334 0.37137099 0.28230206\n",
      " 0.37137099 0.01494612 0.57584953 0.09855052 0.12173698 0.03123112\n",
      " 0.57584953 0.00539399 0.17964952 0.57584953 0.23004181 0.43554295\n",
      " 0.65949972]\n",
      "[0.1790013  0.1790013  0.00474577 0.1790013  0.05554714 0.00474577\n",
      " 0.00474577 0.09902671 0.60765644 0.56914512 0.37184718 0.28165384\n",
      " 0.37184718 0.01542231 0.57632572 0.09902671 0.12108875 0.03170731\n",
      " 0.57632572 0.00474577 0.1790013  0.57632572 0.230518   0.43601914\n",
      " 0.66143808]\n",
      "[0.17826699 0.17826699 0.00401145 0.17826699 0.05481283 0.00401145\n",
      " 0.00401145 0.09945528 0.60808501 0.57038871 0.37227575 0.28091953\n",
      " 0.37227575 0.01585088 0.57675429 0.09945528 0.12035444 0.03213588\n",
      " 0.57675429 0.00401145 0.17826699 0.57675429 0.23094657 0.43644771\n",
      " 0.66268168]\n",
      "[0.17878267 0.17878267 0.00202601 0.17878267 0.05282739 0.00202601\n",
      " 0.00202601 0.099841   0.60847073 0.5709044  0.37266147 0.28143521\n",
      " 0.37266147 0.01623659 0.57714001 0.099841   0.12087013 0.0325216\n",
      " 0.57714001 0.00202601 0.17878267 0.57714001 0.23133229 0.43683343\n",
      " 0.66502208]\n",
      "[ 1.79101720e-01  1.79101720e-01 -5.24748003e-04  1.79101720e-01\n",
      "  5.31464351e-02 -5.24748003e-04 -5.24748003e-04  1.00188141e-01\n",
      "  6.08817871e-01  5.71223441e-01  3.73008612e-01  2.81754255e-01\n",
      "  3.73008612e-01  1.65837364e-02  5.77487149e-01  1.00188141e-01\n",
      "  1.21189173e-01  3.28687401e-02  5.77487149e-01 -5.24748003e-04\n",
      "  1.79101720e-01  5.77487149e-01  2.31679429e-01  4.37180572e-01\n",
      "  6.65341129e-01]\n",
      "[ 0.17960886  0.17960886 -0.00099333  0.17960886  0.05365357 -0.00099333\n",
      " -0.00099333  0.09971956  0.60834929  0.57173058  0.37254003  0.28226139\n",
      "  0.37254003  0.01611515  0.57701857  0.09971956  0.12169631  0.03240016\n",
      "  0.57701857 -0.00099333  0.17960886  0.57701857  0.23121085  0.43671199\n",
      "  0.66584827]\n",
      "[ 0.18006528  0.18006528 -0.00162253  0.18006528  0.05410999 -0.00162253\n",
      " -0.00162253  0.09909036  0.60901452  0.572187    0.37191083  0.28271781\n",
      "  0.37191083  0.01548595  0.5776838   0.09909036  0.12215273  0.03177096\n",
      "  0.5776838  -0.00162253  0.18006528  0.5776838   0.23058164  0.43737722\n",
      "  0.66630469]\n",
      "[ 0.18047606  0.18047606 -0.0024024   0.18047606  0.05452078 -0.0024024\n",
      " -0.0024024   0.09831049  0.60928047  0.57259778  0.37217678  0.2831286\n",
      "  0.37217678  0.01470608  0.57794975  0.09831049  0.12256351  0.03099109\n",
      "  0.57794975 -0.0024024   0.18047606  0.57794975  0.22980178  0.43764317\n",
      "  0.66671547]\n",
      "[ 0.18080631  0.18080631 -0.00464762  0.18080631  0.05485103 -0.00464762\n",
      " -0.00464762  0.09842766  0.60939765  0.57292803  0.37229396  0.28345885\n",
      "  0.37229396  0.01580878  0.57806692  0.09842766  0.12289376  0.03209379\n",
      "  0.57806692 -0.00464762  0.18080631  0.57806692  0.22991895  0.43776035\n",
      "  0.66704572]\n",
      "[ 0.18056827  0.18056827 -0.00488566  0.18056827  0.05461298 -0.00488566\n",
      " -0.00488566  0.09818962  0.60981508  0.57376612  0.37205591  0.2832208\n",
      "  0.37205591  0.01557074  0.57782888  0.09818962  0.12265572  0.03185574\n",
      "  0.57782888 -0.00488566  0.18056827  0.57782888  0.22968091  0.4375223\n",
      "  0.66788381]\n",
      "[ 0.18089288  0.18089288 -0.00626894  0.18089288  0.05493759 -0.00626894\n",
      " -0.00626894  0.0984301   0.61005556  0.57409073  0.37229639  0.28354541\n",
      "  0.37229639  0.01418746  0.57806936  0.0984301   0.12298033  0.03614512\n",
      "  0.57806936 -0.00626894  0.18089288  0.57806936  0.22992138  0.43776278\n",
      "  0.66820842]\n",
      "[ 0.18086776  0.18086776 -0.00629406  0.18086776  0.05491248 -0.00629406\n",
      " -0.00629406  0.09840498  0.60649835  0.57406561  0.37227127  0.2835203\n",
      "  0.37227127  0.01416235  0.57804424  0.09840498  0.12295521  0.03612\n",
      "  0.57804424 -0.00629406  0.18086776  0.57804424  0.22989627  0.43773766\n",
      "  0.6681833 ]\n",
      "[ 0.18113035  0.18113035 -0.00815011  0.18113035  0.05517507 -0.00815011\n",
      " -0.00815011  0.0984768   0.60657018  0.5743282   0.3723431   0.28378289\n",
      "  0.3723431   0.01518422  0.57811607  0.0984768   0.1232178   0.03714188\n",
      "  0.57811607 -0.00815011  0.18113035  0.57811607  0.22996809  0.43780949\n",
      "  0.66844589]\n",
      "[ 0.18075215  0.18075215 -0.00852831  0.18075215  0.05479687 -0.00852831\n",
      " -0.00852831  0.09874943  0.6068428   0.57395     0.37261572  0.28340469\n",
      "  0.37261572  0.01545685  0.57838869  0.09874943  0.12283961  0.0374145\n",
      "  0.57838869 -0.00852831  0.18075215  0.57838869  0.23024072  0.43808212\n",
      "  0.66948964]\n",
      "[ 0.18062969  0.18062969 -0.00865077  0.18062969  0.0546744  -0.00865077\n",
      " -0.00865077  0.09862697  0.60672034  0.57494271  0.37249326  0.28328222\n",
      "  0.37249326  0.01533438  0.57826623  0.09862697  0.12271714  0.03729204\n",
      "  0.57826623 -0.00865077  0.18062969  0.57826623  0.23011825  0.43795965\n",
      "  0.67048235]\n",
      "[ 0.18091164  0.18091164 -0.00906307  0.18091164  0.05495635 -0.00906307\n",
      " -0.00906307  0.09821467  0.60703967  0.57522466  0.37208096  0.28356417\n",
      "  0.37208096  0.01492209  0.57858556  0.09821467  0.12299909  0.03687974\n",
      "  0.57858556 -0.00906307  0.18091164  0.57858556  0.22970596  0.43827899\n",
      "  0.6707643 ]\n",
      "[ 0.18116539  0.18116539 -0.00996651  0.18116539  0.05521011 -0.00996651\n",
      " -0.00996651  0.09824914  0.60707415  0.57547842  0.37211544  0.28381793\n",
      "  0.37211544  0.01401865  0.57862004  0.09824914  0.12325284  0.0359763\n",
      "  0.57862004 -0.00996651  0.18116539  0.57862004  0.22974043  0.43831346\n",
      "  0.67101805]\n",
      "[ 0.1811058   0.1811058  -0.0100261   0.1811058   0.05515052 -0.0100261\n",
      " -0.0100261   0.09529627  0.60716765  0.57541883  0.37220894  0.28375834\n",
      "  0.37220894  0.01395906  0.57871354  0.09818956  0.12319326  0.03591671\n",
      "  0.57871354 -0.0100261   0.1811058   0.57871354  0.23077415  0.43840696\n",
      "  0.67095846]\n",
      "[ 0.18109313  0.18109313 -0.01003877  0.18109313  0.05513785 -0.01003877\n",
      " -0.01003877  0.0952836   0.60389923  0.57540616  0.37219627  0.28374567\n",
      "  0.37219627  0.01394639  0.57870087  0.09817689  0.12318059  0.03590404\n",
      "  0.57870087 -0.01003877  0.18109313  0.57870087  0.23076148  0.43839429\n",
      "  0.6709458 ]\n",
      "[ 0.18079623  0.18079623 -0.01033567  0.18079623  0.05484095 -0.01033567\n",
      " -0.01033567  0.09552401  0.60413964  0.57510926  0.37243668  0.28344877\n",
      "  0.37243668  0.0141868   0.57894128  0.0984173   0.12288368  0.03614445\n",
      "  0.57894128 -0.01033567  0.18079623  0.57894128  0.23100189  0.4386347\n",
      "  0.67064889]\n",
      "[ 0.18110644  0.18110644 -0.01059412  0.18110644  0.05458249 -0.01059412\n",
      " -0.01059412  0.09526556  0.60427523  0.57541946  0.37217823  0.28375897\n",
      "  0.37217823  0.01392835  0.57868283  0.09815885  0.12319389  0.035886\n",
      "  0.57868283 -0.01059412  0.18110644  0.57868283  0.23074344  0.43837625\n",
      "  0.6709591 ]\n",
      "[ 0.18102825  0.18102825 -0.01067231  0.18102825  0.05450431 -0.01067231\n",
      " -0.01067231  0.09526374  0.60427342  0.57534127  0.37217642  0.28368078\n",
      "  0.37217642  0.01485972  0.57868102  0.09815703  0.1231157   0.03681737\n",
      "  0.57868102 -0.01067231  0.18102825  0.57868102  0.23074163  0.43837444\n",
      "  0.67088091]\n",
      "[ 0.18074612  0.18074612 -0.01095443  0.18074612  0.05422218 -0.01095443\n",
      " -0.01095443  0.0954664   0.60447608  0.57505915  0.37237907  0.28339866\n",
      "  0.37237907  0.01506237  0.57888367  0.09835969  0.12283357  0.03702003\n",
      "  0.57888367 -0.01095443  0.18074612  0.57888367  0.23094429  0.4385771\n",
      "  0.67168115]\n",
      "[ 0.18108488  0.18108488 -0.01113036  0.18108488  0.05404626 -0.01113036\n",
      " -0.01113036  0.09529048  0.60430015  0.57539791  0.37220315  0.28373741\n",
      "  0.37220315  0.01488645  0.57870775  0.09818377  0.12265765  0.0368441\n",
      "  0.57870775 -0.01113036  0.18108488  0.57870775  0.23076836  0.43840117\n",
      "  0.67201991]\n",
      "[ 0.1813223   0.1813223  -0.01157675  0.1813223   0.05428368 -0.01157675\n",
      " -0.01157675  0.09484408  0.60432515  0.57563533  0.37222814  0.28397484\n",
      "  0.37222814  0.01444005  0.57873274  0.09773737  0.12289507  0.03639771\n",
      "  0.57873274 -0.01157675  0.1813223   0.57873274  0.23032197  0.43842617\n",
      "  0.67225733]\n",
      "[ 0.18129597  0.18129597 -0.01160308  0.18129597  0.05425735 -0.01160308\n",
      " -0.01160308  0.09481775  0.60455707  0.575609    0.37220182  0.28394851\n",
      "  0.37220182  0.01441373  0.57896467  0.09771104  0.12286875  0.03637138\n",
      "  0.57896467 -0.01160308  0.18129597  0.581736    0.23029564  0.43527877\n",
      "  0.67223101]\n",
      "[ 0.18097075  0.18097075 -0.01192831  0.18097075  0.05393213 -0.01192831\n",
      " -0.01192831  0.09501125  0.60475057  0.57730975  0.37239531  0.28362328\n",
      "  0.37239531  0.01460722  0.57915817  0.09790454  0.12254352  0.03656488\n",
      "  0.57915817 -0.01192831  0.18097075  0.58192949  0.23048914  0.43547226\n",
      "  0.67228821]\n",
      "[ 0.1809686   0.1809686  -0.01193045  0.1809686   0.05392998 -0.01193045\n",
      " -0.01193045  0.0950091   0.60474842  0.5773076   0.37239317  0.28362114\n",
      "  0.37239317  0.01460508  0.57915602  0.09790239  0.12254138  0.03656273\n",
      "  0.57915602 -0.01193045  0.1809686   0.58192735  0.23048699  0.43547012\n",
      "  0.67294773]\n",
      "[ 0.18096741  0.18096741 -0.01193164  0.18096741  0.05392879 -0.01193164\n",
      " -0.01193164  0.09500791  0.60172193  0.57730641  0.37239197  0.28361995\n",
      "  0.37239197  0.01460388  0.57915483  0.0979012   0.12254018  0.03656154\n",
      "  0.57915483 -0.01193164  0.18096741  0.58192616  0.2304858   0.43546892\n",
      "  0.67294653]\n",
      "[ 0.18097331  0.18097331 -0.01192574  0.18097331  0.05393469 -0.01192574\n",
      " -0.01192574  0.09501381  0.60185603  0.57731231  0.37135533  0.28362585\n",
      "  0.37135533  0.01460979  0.57928893  0.0979071   0.12254608  0.03656744\n",
      "  0.57928893 -0.01192574  0.18097331  0.58206026  0.2304917   0.43560303\n",
      "  0.67295244]\n",
      "[ 0.18048649  0.18048649 -0.01241256  0.18048649  0.05344787 -0.01241256\n",
      " -0.01241256  0.09518866  0.60203087  0.57719224  0.37153018  0.28313902\n",
      "  0.37153018  0.01478463  0.57946378  0.09808195  0.12205926  0.03674228\n",
      "  0.57946378 -0.01241256  0.18048649  0.5822351   0.23066654  0.43577787\n",
      "  0.67283237]\n",
      "[ 0.18050311  0.18050311 -0.01239594  0.18050311  0.05346449 -0.01239594\n",
      " -0.01239594  0.09520528  0.6020475   0.57736653  0.3715468   0.28315565\n",
      "  0.3715468   0.01480125  0.5794804   0.09809857  0.12207589  0.03675891\n",
      "  0.5794804  -0.01239594  0.18050311  0.58225173  0.23068317  0.4357945\n",
      "  0.67300665]\n",
      "[ 0.18050971  0.18050971 -0.01560314  0.18050971  0.05347109 -0.01232361\n",
      " -0.01232361  0.09527761  0.60211983  0.57737313  0.37161913  0.28316225\n",
      "  0.37161913  0.01487358  0.57955273  0.0981709   0.12208249  0.03683123\n",
      "  0.57955273 -0.01232361  0.18050971  0.58232406  0.2307555   0.43586682\n",
      "  0.67301325]\n",
      "[ 0.18061897  0.18061897 -0.0156085   0.18149362  0.05346574 -0.01232896\n",
      " -0.01232896  0.09527226  0.60211447  0.57748238  0.37161378  0.28327151\n",
      "  0.37161378  0.01486823  0.57954738  0.09816555  0.12306639  0.03682588\n",
      "  0.57954738 -0.01232896  0.18061897  0.5823187   0.23075014  0.43586147\n",
      "  0.67312251]\n",
      "[ 0.18082685  0.18082685 -0.01583488  0.1817015   0.05367362 -0.01255534\n",
      " -0.01255534  0.09504587  0.60209025  0.57769027  0.37138739  0.28347939\n",
      "  0.37138739  0.01464185  0.579321    0.09793917  0.12327428  0.0365995\n",
      "  0.579321   -0.01255534  0.18082685  0.58209232  0.23052376  0.43563509\n",
      "  0.67333039]\n",
      "[ 0.18063008  0.18063008 -0.01603165  0.18150473  0.05347685 -0.01275212\n",
      " -0.01275212  0.09520449  0.60224886  0.57749349  0.37154601  0.28328261\n",
      "  0.37154601  0.01480046  0.57947961  0.09809778  0.1230775   0.03675811\n",
      "  0.57947961 -0.01275212  0.18063008  0.58225094  0.23068238  0.4357937\n",
      "  0.67313362]\n",
      "[ 0.18064947  0.18064947 -0.01601226  0.18152412  0.05349624 -0.01273273\n",
      " -0.01273273  0.09522388  0.60226825  0.57839177  0.3715654   0.283302\n",
      "  0.3715654   0.01481985  0.579499    0.09811717  0.12309689  0.0367775\n",
      "  0.579499   -0.01273273  0.18064947  0.58227033  0.23070177  0.43581309\n",
      "  0.67310774]\n",
      "[ 0.18063585  0.18063585 -0.01602588  0.1815105   0.05348262 -0.01274635\n",
      " -0.01274635  0.09521026  0.60225464  0.57827834  0.37155178  0.28328839\n",
      "  0.37155178  0.01480623  0.57948538  0.09810355  0.12308327  0.03676389\n",
      "  0.57948538 -0.01274635  0.18063585  0.58225671  0.23068815  0.43579948\n",
      "  0.6729943 ]\n",
      "[ 0.18083785  0.18083785 -0.01619358  0.18171249  0.05368461 -0.01291405\n",
      " -0.01291405  0.09504256  0.60208694  0.57848033  0.37138408  0.28349038\n",
      "  0.37138408  0.01463854  0.57931768  0.09793585  0.12328527  0.03659619\n",
      "  0.57931768 -0.01291405  0.18083785  0.58208901  0.23052045  0.43563178\n",
      "  0.6731963 ]\n",
      "[ 0.18083919  0.18083919 -0.01619224  0.18171383  0.05368595 -0.01291271\n",
      " -0.01291271  0.0950439   0.60208828  0.57848167  0.37138542  0.28349172\n",
      "  0.37138542  0.01463987  0.57931902  0.09793719  0.12328661  0.03659753\n",
      "  0.57931902 -0.01291271  0.18083919  0.58209035  0.23052179  0.43563312\n",
      "  0.67376501]\n",
      "[ 0.18081262  0.18081262 -0.0162188   0.18168727  0.05365939 -0.01293927\n",
      " -0.01293927  0.09496931  0.60201369  0.57845511  0.37131083  0.28346516\n",
      "  0.37131083  0.01550009  0.57924443  0.0978626   0.12326005  0.03745774\n",
      "  0.57924443 -0.01293927  0.18081262  0.58201576  0.2304472   0.43555853\n",
      "  0.67373844]\n",
      "[ 0.18078053  0.18078053 -0.0162509   0.18165517  0.05362729 -0.01297137\n",
      " -0.01297137  0.09123039  0.60197268  0.57842301  0.37126982  0.28343306\n",
      "  0.37126982  0.01546799  0.57920342  0.0978305   0.12322795  0.03742564\n",
      "  0.57920342 -0.01297137  0.18078053  0.58197475  0.2313102   0.43551752\n",
      "  0.67370635]\n",
      "[ 0.1807983   0.1807983  -0.01623312  0.18167295  0.05364507 -0.01295359\n",
      " -0.01295359  0.09124816  0.60199045  0.57852115  0.3712876   0.28345084\n",
      "  0.3712876   0.01548577  0.5792212   0.09784828  0.12324572  0.03744342\n",
      "  0.5792212  -0.01295359  0.1807983   0.58199253  0.23132798  0.43553529\n",
      "  0.67380448]\n",
      "[ 0.18026544  0.18026544 -0.01676599  0.18114009  0.05311221 -0.01284787\n",
      " -0.01284787  0.09135388  0.60209617  0.57841715  0.37139332  0.28291798\n",
      "  0.37139332  0.01559148  0.57932692  0.097954    0.12271286  0.03754914\n",
      "  0.57932692 -0.01284787  0.18026544  0.58209824  0.2314337   0.43564101\n",
      "  0.67370048]\n",
      "[ 0.18026108  0.18026108 -0.02103728  0.18113572  0.05310784 -0.01278358\n",
      " -0.01278358  0.09141817  0.60216046  0.57841278  0.3714576   0.28291361\n",
      "  0.3714576   0.01565577  0.57939121  0.09801829  0.1227085   0.03761342\n",
      "  0.57939121 -0.01278358  0.18026108  0.58216253  0.23149798  0.4357053\n",
      "  0.67369612]\n",
      "[ 0.18027541  0.18027541 -0.02102295  0.18115006  0.05312218 -0.01276925\n",
      " -0.01276925  0.0914325   0.60217479  0.57983315  0.37147194  0.28292794\n",
      "  0.37147194  0.0156701   0.57940554  0.09803262  0.12272283  0.03762776\n",
      "  0.57940554 -0.01276925  0.18027541  0.58217686  0.23151232  0.43571963\n",
      "  0.67367407]\n",
      "[ 0.18026513  0.18026513 -0.02103323  0.18113977  0.05311189 -0.01277953\n",
      " -0.01277953  0.09142222  0.60216451  0.57974931  0.37146166  0.28291766\n",
      "  0.37146166  0.01565982  0.57939526  0.09802234  0.12271255  0.03761748\n",
      "  0.57939526 -0.01277953  0.18026513  0.58216658  0.23150204  0.43570935\n",
      "  0.67359023]\n",
      "[ 0.18033934  0.18033934 -0.02183238  0.18463447  0.05231275 -0.01269122\n",
      " -0.01269122  0.09151053  0.60225283  0.57982352  0.37154997  0.28299188\n",
      "  0.37154997  0.01574814  0.57948357  0.09811065  0.1219134   0.03770579\n",
      "  0.57948357 -0.01269122  0.18033934  0.5822549   0.23159035  0.43579766\n",
      "  0.67366444]\n",
      "[ 0.18033575  0.18033575 -0.02403409  0.18463089  0.05230916 -0.01261174\n",
      " -0.01261174  0.09159002  0.60233231  0.57981993  0.37162945  0.28298829\n",
      "  0.37162945  0.01582762  0.57956305  0.09819013  0.12190981  0.03778527\n",
      "  0.57956305 -0.01261174  0.18033575  0.58233438  0.23166983  0.43587715\n",
      "  0.67366085]\n",
      "[ 0.18034845  0.18034845 -0.02402139  0.18464358  0.05232186 -0.01259904\n",
      " -0.01259904  0.09160271  0.602345    0.57991352  0.37164215  0.28300098\n",
      "  0.37164215  0.01584032  0.57957575  0.09820283  0.12192251  0.03779797\n",
      "  0.57957575 -0.01259904  0.18034845  0.58234708  0.23168253  0.43588984\n",
      "  0.67375444]\n",
      "[ 0.18041934  0.18041934 -0.02402092  0.18553614  0.05232233 -0.01259857\n",
      " -0.01259857  0.09160318  0.60234547  0.57998442  0.37164262  0.28307188\n",
      "  0.37164262  0.01584079  0.57957622  0.0982033   0.12281507  0.03779844\n",
      "  0.57957622 -0.01259857  0.18041934  0.58234755  0.231683    0.43589031\n",
      "  0.67382534]\n",
      "[ 0.1804259   0.1804259  -0.02401436  0.1855427   0.05232889 -0.01259201\n",
      " -0.01259201  0.09160974  0.59955927  0.57999098  0.37164918  0.28307844\n",
      "  0.37164918  0.01584735  0.57958278  0.09820986  0.12282163  0.037805\n",
      "  0.57958278 -0.01259201  0.1804259   0.58235411  0.23168956  0.43589687\n",
      "  0.6738319 ]\n",
      "[ 0.1804245   0.1804245  -0.02756082  0.18554129  0.05232748 -0.01254517\n",
      " -0.01254517  0.09165658  0.59960611  0.57998958  0.37169601  0.28307704\n",
      "  0.37169601  0.01589418  0.57962961  0.0982567   0.12282022  0.03785183\n",
      "  0.57962961 -0.01254517  0.1804245   0.58240094  0.23173639  0.43594371\n",
      "  0.6738305 ]\n",
      "[ 0.18042598  0.18042598 -0.02755934  0.18554277  0.05232896 -0.0125437\n",
      " -0.0125437   0.09165806  0.59960758  0.57999105  0.37169749  0.28307851\n",
      "  0.37169749  0.01589566  0.57963109  0.09825817  0.1228217   0.03785331\n",
      "  0.57963109 -0.0125437   0.18042598  0.58240242  0.23173787  0.43594519\n",
      "  0.67433578]\n",
      "[ 0.18041893  0.18041893 -0.0275664   0.18553572  0.05232191 -0.01255075\n",
      " -0.01255075  0.09165101  0.59952009  0.579984    0.37169044  0.28307146\n",
      "  0.37169044  0.01588861  0.57848966  0.09825112  0.12281465  0.03784626\n",
      "  0.57848966 -0.01255075  0.18041893  0.58239537  0.23173082  0.43593814\n",
      "  0.67432873]\n",
      "[ 0.18040954  0.18040954 -0.02757578  0.18552634  0.05231253 -0.01256013\n",
      " -0.01256013  0.09164162  0.59951071  0.58074131  0.37168106  0.28306208\n",
      "  0.37168106  0.01587923  0.57848028  0.09824174  0.12280527  0.03783688\n",
      "  0.57848028 -0.01256013  0.18040954  0.58238599  0.23172144  0.43592875\n",
      "  0.6742975 ]\n",
      "[ 0.18032351  0.18032351 -0.02766181  0.1854403   0.05222649 -0.01264616\n",
      " -0.01264616  0.09155559  0.59942468  0.58100297  0.37159502  0.28332373\n",
      "  0.37159502  0.01579319  0.57839424  0.09815571  0.12271923  0.03775084\n",
      "  0.57839424 -0.01264616  0.18032351  0.58229995  0.2316354   0.43584272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.67455915]\n",
      "[ 0.18034942  0.18034942 -0.0276359   0.18546622  0.05225241 -0.01262025\n",
      " -0.01262025  0.0915815   0.59943821  0.58102888  0.37273007  0.28334965\n",
      "  0.37273007  0.0158191   0.57840777  0.09818162  0.12274515  0.03777676\n",
      "  0.57840777 -0.01262025  0.18034942  0.58231348  0.23166132  0.43585625\n",
      "  0.67458507]\n",
      "[ 0.18041212  0.18041212 -0.0275732   0.18552891  0.0523151  -0.01255755\n",
      " -0.01255755  0.0908784   0.59937013  0.58109157  0.37266198  0.28341234\n",
      "  0.37266198  0.0158818   0.57833969  0.09747851  0.12280784  0.03783945\n",
      "  0.57833969 -0.01255755  0.18041212  0.5822454   0.23095821  0.43578817\n",
      "  0.67464776]\n",
      "[ 0.1804092   0.1804092  -0.02757612  0.18552599  0.05231218 -0.01256048\n",
      " -0.01256048  0.09087548  0.59942281  0.58108865  0.37265906  0.28340942\n",
      "  0.37265906  0.01587888  0.57839238  0.09747559  0.12280492  0.03783653\n",
      "  0.57839238 -0.01256048  0.1804092   0.58489739  0.23095529  0.43290457\n",
      "  0.67464484]\n",
      "[ 0.18039922  0.18039922 -0.0275861   0.18551602  0.0523022  -0.01257045\n",
      " -0.01257045  0.09085337  0.59940071  0.58107868  0.37263696  0.28339944\n",
      "  0.37263696  0.0158689   0.57837027  0.09745349  0.12279494  0.03782656\n",
      "  0.57837027 -0.01257045  0.18039922  0.58487529  0.23093319  0.43288246\n",
      "  0.67463486]\n",
      "[ 0.18054975  0.18054975 -0.02770674  0.18566654  0.05245273 -0.01269109\n",
      " -0.01269109  0.09073273  0.59928007  0.5812292   0.37251632  0.28354997\n",
      "  0.37251632  0.01574826  0.57824963  0.09733285  0.12294547  0.03770591\n",
      "  0.57824963 -0.01269109  0.18054975  0.58475465  0.23081255  0.43276182\n",
      "  0.67478539]\n",
      "[ 0.1805614   0.1805614  -0.02769509  0.18567819  0.05246438 -0.01267944\n",
      " -0.01267944  0.08834615  0.59922192  0.58124085  0.37245817  0.28356162\n",
      "  0.37245817  0.01575992  0.57819148  0.0973445   0.12295712  0.03771757\n",
      "  0.57819148 -0.01267944  0.1805614   0.58469649  0.23155409  0.43270367\n",
      "  0.67479704]\n",
      "[ 0.18058064  0.18058064 -0.02767584  0.18569744  0.05248363 -0.0126602\n",
      " -0.0126602   0.0883654   0.59924183  0.5812601   0.37158439  0.28358087\n",
      "  0.37158439  0.01577916  0.57821139  0.09736375  0.12297637  0.03773681\n",
      "  0.57821139 -0.0126602   0.18058064  0.5847164   0.23157333  0.43272358\n",
      "  0.67481629]\n",
      "[ 0.18062944  0.18062944 -0.02762704  0.18574624  0.05253243 -0.0126114\n",
      " -0.0126114   0.08774825  0.59919605  0.5813089   0.37153862  0.28362967\n",
      "  0.37153862  0.01582796  0.57816562  0.0967466   0.12302517  0.03778561\n",
      "  0.57816562 -0.0126114   0.18062944  0.58467063  0.23095619  0.43267781\n",
      "  0.67486509]\n",
      "[ 0.18060753  0.18060753 -0.02764896  0.18572433  0.05251051 -0.01263331\n",
      " -0.01263331  0.08767453  0.59912233  0.58128699  0.3714649   0.28360775\n",
      "  0.3714649   0.01580605  0.5780919   0.09667288  0.12300325  0.0377637\n",
      "  0.5780919  -0.01263331  0.18060753  0.58459691  0.23088246  0.43260409\n",
      "  0.67484317]\n",
      "[ 0.18065901  0.18065901 -0.02759748  0.18577581  0.052562   -0.01258183\n",
      " -0.01342042  0.08770103  0.59914883  0.58133847  0.3714914   0.28365923\n",
      "  0.3714914   0.01501893  0.5781184   0.09669938  0.12305474  0.04146228\n",
      "  0.5781184  -0.01258183  0.18065901  0.58462341  0.23090896  0.43263059\n",
      "  0.67489466]\n",
      "[ 0.18065266  0.18065266 -0.02760383  0.18576945  0.05255564 -0.01258818\n",
      " -0.01342678  0.08769467  0.59904794  0.58133211  0.37148504  0.28365288\n",
      "  0.37148504  0.01501258  0.57968869  0.09669302  0.12304838  0.04145592\n",
      "  0.57590812 -0.01258818  0.18065266  0.58461705  0.23090261  0.43262423\n",
      "  0.6748883 ]\n",
      "[ 0.18065914  0.18065914 -0.02759735  0.18577594  0.05256212 -0.0125817\n",
      " -0.01342029  0.08770116  0.59659149  0.5813386   0.37149153  0.28365936\n",
      "  0.37149153  0.01501906  0.57969518  0.09669951  0.12305487  0.04146241\n",
      "  0.5759146  -0.0125817   0.18065914  0.58462354  0.23090909  0.43263072\n",
      "  0.67489478]\n",
      "[ 0.18067393  0.18067393 -0.02758256  0.18579072  0.05257691 -0.01256691\n",
      " -0.01340551  0.08771594  0.5966188   0.58135338  0.37227881  0.28367415\n",
      "  0.37227881  0.01503385  0.57972248  0.09671429  0.12306965  0.04147719\n",
      "  0.57594191 -0.01256691  0.18067393  0.58465084  0.23092388  0.43265802\n",
      "  0.67490957]\n",
      "[ 0.18065316  0.18065316 -0.02760333  0.18576995  0.05255614 -0.01258768\n",
      " -0.01342627  0.08470236  0.59658741  0.58133261  0.37224742  0.28365338\n",
      "  0.37224742  0.01501308  0.5796911   0.09669352  0.12304888  0.04145642\n",
      "  0.57591052 -0.01258768  0.18065316  0.58461946  0.23157896  0.43262664\n",
      "  0.6748888 ]\n",
      "[ 0.1806489   0.1806489  -0.02760759  0.18576569  0.05255188 -0.01259194\n",
      " -0.01343054  0.08469809  0.59664116  0.58132835  0.37224316  0.28364912\n",
      "  0.37224316  0.01500882  0.57974485  0.09668926  0.12304462  0.04145216\n",
      "  0.57596427 -0.01259194  0.1806489   0.58703404  0.23157469  0.43005919\n",
      "  0.67488454]\n",
      "[ 0.18064312  0.18064312 -0.02761337  0.18575991  0.0525461  -0.01259772\n",
      " -0.01343631  0.08469232  0.59655447  0.58132257  0.37223738  0.28364334\n",
      "  0.37223738  0.01500304  0.57878107  0.09668348  0.12303884  0.04144639\n",
      "  0.57500049 -0.01259772  0.18064312  0.58702827  0.23156892  0.43005342\n",
      "  0.67487876]\n"
     ]
    }
   ],
   "source": [
    "for i in gbrt.staged_predict(X_val):\n",
    "    print(i)\n",
    "#it returns an iterator over the predictions made by the ensemble at each stage of training (with one tree, two trees, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c19fdf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=55, random_state=42)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b58137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b9326bf",
   "metadata": {},
   "source": [
    "It is also possible to implement early stopping by actually stopping training early\n",
    "(instead of training a large number of trees first and then looking back to find the\n",
    "optimal number). You can do so by setting warm_start=True, which makes Scikit\u0002Learn keep existing trees when the fit() method is called, allowing incremental\n",
    "training. The following code stops training when the validation error does not\n",
    "improve for five iterations in a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea5c9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f37bc9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "print(gbrt.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8eaa6feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation MSE: 0.002712853325235463\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum validation MSE:\", min_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd16e90",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Boosting\n",
    "The GradientBoostingRegressor class also supports a subsample hyperparameter,\n",
    "which specifies the fraction of training instances to be used for training each tree. For\n",
    "example, if subsample=0.25, then each tree is trained on 25% of the training instan‐\n",
    "ces, selected randomly. As you can probably guess by now, this trades a higher bias\n",
    "for a lower variance. It also speeds up training considerably. This technique is called\n",
    "Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115ff49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5711d0fe",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "It is worth noting that an optimized implementation of Gradient Boosting is available\n",
    "in the popular python library XGBoost, which stands for Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43f28f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.1-py3-none-win_amd64.whl (125.4 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from xgboost) (1.20.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a4aa68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 0.004000408205406276\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "\n",
    "val_error = mean_squared_error(y_val, y_pred) # Not shown\n",
    "print(\"Validation MSE:\", val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7a7382e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:793: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.22834\n",
      "[1]\tvalidation_0-rmse:0.16224\n",
      "[2]\tvalidation_0-rmse:0.11843\n",
      "[3]\tvalidation_0-rmse:0.08760\n",
      "[4]\tvalidation_0-rmse:0.06848\n",
      "[5]\tvalidation_0-rmse:0.05709\n",
      "[6]\tvalidation_0-rmse:0.05297\n",
      "[7]\tvalidation_0-rmse:0.05129\n",
      "[8]\tvalidation_0-rmse:0.05155\n",
      "Validation MSE: 0.002630868681577655\n"
     ]
    }
   ],
   "source": [
    "#XGBoost also offers several nice features, such as automatically taking care of early stopping\n",
    "\n",
    "xgb_reg.fit(X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "val_error = mean_squared_error(y_val, y_pred)  # Not shown\n",
    "print(\"Validation MSE:\", val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56571019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425 ms ± 87.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit xgboost.XGBRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32bae71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 ms ± 29.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit GradientBoostingRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a9bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
